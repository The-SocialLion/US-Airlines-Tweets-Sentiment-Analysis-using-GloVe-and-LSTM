# -*- coding: utf-8 -*-
"""USASA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1WH7Jf73tU5OFgu7Xdkw6MFllDa9z-T

## US Airlines Tweet Sentiment Analysis using GloVe (Global Vectors for Word Representation) and LSTM
"""

import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("Tweets.csv")
df=df.drop(columns=['tweet_id','negativereason','airline','retweet_count','airline_sentiment_confidence','negativereason_confidence','airline_sentiment_gold','name','negativereason_gold','tweet_coord','tweet_created','tweet_location','user_timezone'])
df['airline_sentiment']=df['airline_sentiment'].replace('neutral','positive')
df=df.dropna(how='any')
df

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['sentiment']=le.fit_transform(df['airline_sentiment'])
df=df.drop(columns=['airline_sentiment'])
df

x = df["sentiment"].value_counts()
plt.grid()
sns.barplot(x.index, x)
plt.gca().set_ylabel("samples")
plt.title("distribution")

plt.grid()

plt.hist(df[df["sentiment"] == 1]["text"].str.len())
plt.title("Airline tweets length")

plt.grid()

plt.hist(df[df["sentiment"] == 0]["text"].str.len())
plt.title("Airline tweets length")

plt.grid()

word1 = df[df["sentiment"] == 1]["text"].str.split().apply(lambda x:[len(i) for i in x])
sns.distplot(word1.map(lambda x: np.mean(x)))
plt.title("Airline tweets length")

plt.grid()

word1 = df[df["sentiment"] == 0]["text"].str.split().apply(lambda x:[len(i) for i in x])
sns.distplot(word1.map(lambda x: np.mean(x)))
plt.title("Airline tweets length")

from collections import defaultdict
from nltk.corpus import stopwords
from nltk.util import ngrams

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))

def create_corpus(sent):
    corpus = []
    for x in df[df["sentiment"] == sent]["text"].str.split():
        print(x)
        for i in x:
            corpus.append(i)
            
    return corpus

corpus = create_corpus(0)

stop = set(stopwords.words("english"))

dictionary = defaultdict(int)
for word in corpus:
    if word in stop:
        dictionary[word] +=1
        
top = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]

x,y = zip(*dictionary.items())

plt.grid()
plt.bar(x,y, color = 'r')
plt.title("Punctuation airline sentiment 0")

"""#**Common words**"""

from collections import Counter

counter = Counter(corpus)
most = counter.most_common()
x = []
y = []

for word, count in most[:40]:
    if word not in stop:
        x.append(word)
        y.append(count)

plt.title("most common words")
plt.grid()
sns.barplot(x = y, y = x)

df

"""#**Removing URLs**"""

import re

def remove_url(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'', text)

df["text"] = df["text"].apply(lambda x: remove_url(x))

df

"""#**Removing HTML**"""

def remove_html(text):
    html = re.compile(r'<.*?>')
    return html.sub(r'', text)

df["text"] = df["text"].apply(lambda x: remove_html(x))

df

"""#**Removing Emojis**"""

def remove_emoji(text):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F" #emoticons
                               u"\U0001F300-\U0001F5FF" #symbols&pics
                               u"\U0001F680-\U0001F6FF" #transportation pic
                               u"\U0001F1E0-\U0001F1FF" #flags
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"    
                               "]+", flags = re.UNICODE)
    return emoji_pattern.sub(r'', text)

df["text"] = df["text"].apply(lambda x: remove_emoji(x))

df

"""#**Removing Punctuation**"""

import string

def remove_punctuation(text):
    table = str.maketrans('', '', string.punctuation)
    return text.translate(table)

df["text"] = df["text"].apply(lambda x: remove_punctuation(x))

df

"""#**Correction of Spellings**"""

pip install pyspellchecker

from spellchecker import SpellChecker

spell = SpellChecker()

def correct_spellings(text):
    corrected_text = []
    
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)

#df['text']=df['text'].apply(lambda x : correct_spellings(x))

df

"""#**Model Construction**"""

from tqdm import tqdm
from nltk.tokenize import word_tokenize
nltk.download('punkt')

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout
from keras.initializers import Constant
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam

def create_corpus(df):
    corpus = []
    for tweet in tqdm(df["text"]):
        words = [word.lower() for word in word_tokenize(tweet) if \
        ((word.isalpha() == 1) & (word not in stop))]
        corpus.append(words)
        
    return corpus

corpus = create_corpus(df)

"""#**Including Glove model**"""

embedding_dict = {}

with open('/content/drive/MyDrive/glove.6B.100d.txt','r') as glove:
    for line in glove:
        values = line.split()
        word = values[0]
        vectors = np.asarray(values[1:], 'float32')
        embedding_dict[word] = vectors
        
glove.close()

MAX_LEN = 50
tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(corpus)

sequences = tokenizer_obj.texts_to_sequences(corpus)

tweet_pad = pad_sequences(sequences,
                          maxlen = MAX_LEN, 
                         truncating = 'post', 
                         padding = 'post')

word_index = tokenizer_obj.word_index
print('number of unique words: ', len(word_index))

num_words = len(word_index) + 1
embedding_matrix = np.zeros((num_words,100))


for word, i in tqdm(word_index.items()):
    if i > num_words:
        continue
        
    embedding_vector = embedding_dict.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

from keras import regularizers

model = Sequential()

glove_embedding = Embedding(num_words, 100, embeddings_initializer = Constant(embedding_matrix), 
                     input_length = MAX_LEN, 
                     trainable = False)

model.add(glove_embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))
model.add(Dense(128, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))
model.add(Dense(256, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))
model.add(Dropout(0.2))
model.add(Dense(1, activation = 'sigmoid'))

optimizer = Adam(learning_rate=1e-5)

model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ["accuracy"])

model.summary()

train_data = tweet_pad[:df.shape[0]]
test_data = tweet_pad[df.shape[0]:]

train_data

test_data

X_train, X_test, y_train, y_test = train_test_split(train_data, df["sentiment"].values, test_size = 0.20)

hist = model.fit(X_train, y_train, batch_size = 1640, epochs = 50, validation_data = (X_test, y_test))

#Ploting Acuracy & Loss
import matplotlib.pyplot as plt
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title("Model Accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Accuracy","Validation Accuracy","loss","Validation Loss"])
plt.show()